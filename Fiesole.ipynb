{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiesole\n",
    "\n",
    "This project aims to demonstrate the feasibility of Vertical-Federated-Learning CNN architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/cho/Downloads/fiesole/Fiesole.ipynb Cell 2\u001b[0m line \u001b[0;36m2\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=206'>207</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ALL_HANDS_ON_DECK):\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=207'>208</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIteration \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=208'>209</a>\u001b[0m     model \u001b[39m=\u001b[39m federated_learning(model)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m \u001b[39m# Plot the results\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=212'>213</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(tl_history, label\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTL\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/cho/Downloads/fiesole/Fiesole.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=182'>183</a>\u001b[0m model_br \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(original_model)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=183'>184</a>\u001b[0m optimizer_br \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model_br\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=185'>186</a>\u001b[0m train_model(model_tl, trainloader_tl, optimizer_tl)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m tl_history\u001b[39m.\u001b[39mappend(test_model(\u001b[39m\"\u001b[39m\u001b[39mTL\u001b[39m\u001b[39m\"\u001b[39m, model_tl))\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m train_model(model_tr, trainloader_tr, optimizer_tr)\n",
      "\u001b[1;32m/Users/cho/Downloads/fiesole/Fiesole.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=151'>152</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/cho/Downloads/fiesole/Fiesole.ipynb#W1sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[0;32m~/Downloads/fiesole/.conda/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/fiesole/.conda/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/Downloads/fiesole/.conda/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Downloads/fiesole/.conda/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m~/Downloads/fiesole/.conda/lib/python3.10/site-packages/torch/optim/adam.py:344\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    341\u001b[0m     param \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mview_as_real(param)\n\u001b[1;32m    343\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    345\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "MNIST_CLASSES = 10\n",
    "LEARNING_RATE = 0.001\n",
    "ALL_HANDS_ON_DECK = 100\n",
    "\n",
    "\n",
    "class QuadrantTransform:\n",
    "    def __init__(self, quadrant):\n",
    "        assert quadrant in [\n",
    "            \"tl\",\n",
    "            \"tr\",\n",
    "            \"bl\",\n",
    "            \"br\",\n",
    "        ], \"Invalid quadrant. Choose from 'tl', 'tr', 'bl', 'br'\"\n",
    "        self.quadrant = quadrant\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Convert PIL image to tensor\n",
    "        tensor = transforms.ToTensor()(img)\n",
    "\n",
    "        # Create a blank (transparent) tensor of the same shape\n",
    "        blank = torch.zeros_like(tensor)\n",
    "\n",
    "        if self.quadrant == \"tl\":\n",
    "            blank[:, :14, :14] = tensor[:, :14, :14]\n",
    "        elif self.quadrant == \"tr\":\n",
    "            blank[:, :14, 14:] = tensor[:, :14, 14:]\n",
    "        elif self.quadrant == \"bl\":\n",
    "            blank[:, 14:, :14] = tensor[:, 14:, :14]\n",
    "        elif self.quadrant == \"br\":\n",
    "            blank[:, 14:, 14:] = tensor[:, 14:, 14:]\n",
    "\n",
    "        # Normalize after applying the quadrant transformation\n",
    "        blank = (blank - 0.5) / 0.5\n",
    "\n",
    "        return blank\n",
    "\n",
    "\n",
    "transform_tl = transforms.Compose([QuadrantTransform(\"tl\")])\n",
    "transform_tr = transforms.Compose([QuadrantTransform(\"tr\")])\n",
    "transform_bl = transforms.Compose([QuadrantTransform(\"bl\")])\n",
    "transform_br = transforms.Compose([QuadrantTransform(\"br\")])\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "trainset_tl = torchvision.datasets.MNIST(\n",
    "    root=\"./control\", train=True, download=True, transform=transform_tl\n",
    ")\n",
    "trainset_tr = torchvision.datasets.MNIST(\n",
    "    root=\"./control\", train=True, download=True, transform=transform_tr\n",
    ")\n",
    "trainset_bl = torchvision.datasets.MNIST(\n",
    "    root=\"./control\", train=True, download=True, transform=transform_bl\n",
    ")\n",
    "trainset_br = torchvision.datasets.MNIST(\n",
    "    root=\"./control\", train=True, download=True, transform=transform_br\n",
    ")\n",
    "\n",
    "trainloader_tl = torch.utils.data.DataLoader(trainset_tl, batch_size=4)\n",
    "trainloader_tr = torch.utils.data.DataLoader(trainset_tr, batch_size=4)\n",
    "trainloader_bl = torch.utils.data.DataLoader(trainset_bl, batch_size=4)\n",
    "trainloader_br = torch.utils.data.DataLoader(trainset_br, batch_size=4)\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root=\"./control\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc_relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc_relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNN()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def test_model(debug_string, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(debug_string, correct / total)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, trainloader, optimizer):\n",
    "    num_epochs = 1\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(trainloader):\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def average_weights(*models):\n",
    "    \"\"\"Averages the weights of the given models.\"\"\"\n",
    "    avg_dict = {}\n",
    "\n",
    "    # Get the state dictionary of the first model to initialize the avg_dict\n",
    "    for key in models[0].state_dict().keys():\n",
    "        avg_dict[key] = sum([model.state_dict()[key] for model in models]) / len(models)\n",
    "\n",
    "    return avg_dict\n",
    "\n",
    "\n",
    "import copy\n",
    "\n",
    "tl_history = []\n",
    "tr_history = []\n",
    "bl_history = []\n",
    "br_history = []\n",
    "avg_history = []\n",
    "\n",
    "\n",
    "def federated_learning(original_model):\n",
    "    model_tl = copy.deepcopy(original_model)\n",
    "    optimizer_tl = optim.Adam(model_tl.parameters(), lr=LEARNING_RATE)\n",
    "    model_tr = copy.deepcopy(original_model)\n",
    "    optimizer_tr = optim.Adam(model_tr.parameters(), lr=LEARNING_RATE)\n",
    "    model_bl = copy.deepcopy(original_model)\n",
    "    optimizer_bl = optim.Adam(model_bl.parameters(), lr=LEARNING_RATE)\n",
    "    model_br = copy.deepcopy(original_model)\n",
    "    optimizer_br = optim.Adam(model_br.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    train_model(model_tl, trainloader_tl, optimizer_tl)\n",
    "    tl_history.append(test_model(\"TL\", model_tl))\n",
    "\n",
    "    train_model(model_tr, trainloader_tr, optimizer_tr)\n",
    "    tr_history.append(test_model(\"TR\", model_tr))\n",
    "\n",
    "    train_model(model_bl, trainloader_bl, optimizer_bl)\n",
    "    bl_history.append(test_model(\"BL\", model_bl))\n",
    "\n",
    "    train_model(model_br, trainloader_br, optimizer_br)\n",
    "    br_history.append(test_model(\"BR\", model_br))\n",
    "\n",
    "    avg_state_dict = average_weights(model_tl, model_tr, model_bl, model_br)\n",
    "\n",
    "    new_model = CNN()\n",
    "    new_model.load_state_dict(avg_state_dict)\n",
    "\n",
    "    avg_history.append(test_model(\"avg\", new_model))\n",
    "    return new_model\n",
    "\n",
    "\n",
    "for i in range(ALL_HANDS_ON_DECK):\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    model = federated_learning(model)\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(tl_history, label=\"TL\")\n",
    "plt.plot(tr_history, label=\"TR\")\n",
    "plt.plot(bl_history, label=\"BL\")\n",
    "plt.plot(br_history, label=\"BR\")\n",
    "plt.plot(avg_history, label=\"avg\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Federations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Project Fiesole\")\n",
    "plt.savefig(\"fiesole.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), \"fiesole.ckpt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
